{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24ff59b",
   "metadata": {},
   "source": [
    "# Print Random 5 questions with their answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69c4b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7108, 3)\n",
      "==============================\n",
      "Random Question 1 : \n",
      " What host compilers are supported by CUDA 11?\n",
      "The Answer:  CUDA 11 supports host compilers including PGI, gcc, clang, Arm, and Microsoft Visual Studio. It also supports experimental host compilers using the --allow-unsupported-compiler flag.\n",
      "==============================\n",
      "==============================\n",
      "Random Question 2 : \n",
      " How has CUDA helped researchers evaluate nanoporous material structures?\n",
      "The Answer:  CUDA has enabled researchers to computationally evaluate large databases of nanoporous material structures efficiently.\n",
      "==============================\n",
      "==============================\n",
      "Random Question 3 : \n",
      " What is the significance of a CUDA-aware MPI implementation in GPU clusters?\n",
      "The Answer:  A CUDA-aware MPI implementation optimizes GPU-to-GPU communication, reducing data transfer overhead and enhancing overall GPU cluster performance in high-performance computing.\n",
      "==============================\n",
      "==============================\n",
      "Random Question 4 : \n",
      " What are some of the challenges faced by LIGO in detecting gravitational waves?\n",
      "The Answer:  LIGO faces challenges such as noise sources (e.g., thermal fluctuations), external disturbances (e.g., traffic), and potential interferences (e.g., rifle fire) in detecting gravitational waves.\n",
      "==============================\n",
      "==============================\n",
      "Random Question 5 : \n",
      " What is the main advantage of using a QR decomposition in the Longstaff-Schwartz algorithm?\n",
      "The Answer:  Using a QR decomposition reduces the computational complexity of the SVD for long-and-thin matrices, resulting in improved performance in option pricing.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "data = pd.read_csv(\"NvidiaDocumentationQandApairs.csv\")\n",
    "print(data.shape)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"=\"*30)\n",
    "    random_question = random.choice(list(data[\"question\"]))\n",
    "    print(\"Random Question\", str(i+1), \": \\n\", random_question)\n",
    "    print(\"The Answer: \", list(data[\"answer\"][data[\"question\"] == random_question])[0])\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcd2e3",
   "metadata": {},
   "source": [
    "# Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ce2420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "model_dir = \"./peft-Nvidia-chatbot-checkpoint-local\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \n",
    "                                       model_dir, \n",
    "                                       torch_dtype=torch.float32,\n",
    "                                       is_trainable=False)\n",
    "\n",
    "\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", \n",
    "                                                       torch_dtype=torch.float32,)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3f53877",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: what is cuda\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: a genus of flowering plants\n",
      "\n",
      "T5 Fine-Tuned:  cuda is a library of deep learning and machine learning classes that provides a high-level interface for analyzing and analyzing data.\n",
      "==============================\n",
      "You: why nvidia insight is useful? \n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: nvidia insight is useful for a variety of reasons\n",
      "\n",
      "T5 Fine-Tuned:  nvidia insight is useful for identifying performance bottlenecks and optimizing code for specific tasks. It provides insights into the performance and reliability of various GPU applications, allowing developers to make better use of the insights.\n",
      "==============================\n",
      "You: is cuda difficult to install ?\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: no\n",
      "\n",
      "T5 Fine-Tuned:  no\n",
      "==============================\n",
      "You: how can we accelerate gpu training with cuda\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: Using a gpu based training program\n",
      "\n",
      "T5 Fine-Tuned:  cuda can accelerate gpu training by enabling asynchronous gpu computations, allowing for efficient data transfers between the CPU and GPU.\n",
      "==============================\n",
      "You: what is nvidia insight\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: a software tool\n",
      "\n",
      "T5 Fine-Tuned:  nvidia insight is a software development environment that allows developers to understand and optimize their applications using NVIDIA GPUs. It provides insights into the performance and performance of GPU-accelerated applications.\n",
      "==============================\n",
      "You: What are some of the challenges faced by LIGO in detecting gravitational waves?\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: The LIGO detector is unable to detect the gravitational wave due to the sensitivity of the detector to light.\n",
      "\n",
      "T5 Fine-Tuned:  LIGO faces challenges related to the difficulty of detecting gravitational waves due to the difficulty of detecting a large number of gravitational waves. It has been trained on a large number of gravitational waves, and the accuracy of the detection is limited by the accuracy of the detector.\n",
      "==============================\n",
      "You: What is cuda 11\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: a saber\n",
      "\n",
      "T5 Fine-Tuned:  cuda 11 is a library of deep learning and machine learning tools for accelerating the development of deep learning inferences. It is a companion to the jQuery library and is optimized for accelerating inferences.\n",
      "==============================\n",
      "You: why parallelization can speed up the training\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: parallelization is a way to speed up the training process\n",
      "\n",
      "T5 Fine-Tuned:  Parallelization can speed up training by allowing the training to be completed in a single timeframe, reducing the time spent on training and improving overall performance.\n",
      "==============================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     start_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m     end_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1187\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1230\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text = input(\"You: \")\n",
    "    start_prompt = '\\n\\n'\n",
    "    end_prompt = '\\n\\nAnswer: '\n",
    "    prompt = start_prompt + text + end_prompt\n",
    "    tokenized_statement = tokenizer([prompt], return_tensors= 'pt')\n",
    "    \n",
    "    \n",
    "\n",
    "    ## Fine tuned model\n",
    "    outputs_ft = peft_model.generate(input_ids=tokenized_statement[\"input_ids\"], \n",
    "                                  generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),\n",
    "                                    )\n",
    "    \n",
    "   \n",
    "    final_output_ft = tokenizer.decode(outputs_ft[0], skip_special_tokens=True)\n",
    "    ###############\n",
    "    \n",
    "    ## Original model\n",
    "    op = original_model.generate(input_ids=tokenized_statement[\"input_ids\"], \n",
    "                                      generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),\n",
    "                                        )\n",
    "    \n",
    "    \n",
    "    print(\"\\nAnswers: \")\n",
    "    print(\"\\nOriginal Model:\", tokenizer.decode(op[0], skip_special_tokens=True))\n",
    "\n",
    "    print(\"\\nT5 Fine-Tuned: \", final_output_ft)\n",
    "    print(\"=\"*30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8305761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
