{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c90cc34",
   "metadata": {},
   "source": [
    "# Print Random 5 questions with their answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ae9d669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7108, 3)\n",
      "==============================\n",
      "Random Question 1 : \n",
      " What host compilers are supported by CUDA 11?\n",
      "The Answer:  CUDA 11 supports host compilers including PGI, gcc, clang, Arm, and Microsoft Visual Studio. It also supports experimental host compilers using the --allow-unsupported-compiler flag.\n",
      "==============================\n",
      "==============================\n",
      "Random Question 2 : \n",
      " How has CUDA helped researchers evaluate nanoporous material structures?\n",
      "The Answer:  CUDA has enabled researchers to computationally evaluate large databases of nanoporous material structures efficiently.\n",
      "==============================\n",
      "==============================\n",
      "Random Question 3 : \n",
      " What is the significance of a CUDA-aware MPI implementation in GPU clusters?\n",
      "The Answer:  A CUDA-aware MPI implementation optimizes GPU-to-GPU communication, reducing data transfer overhead and enhancing overall GPU cluster performance in high-performance computing.\n",
      "==============================\n",
      "==============================\n",
      "Random Question 4 : \n",
      " What are some of the challenges faced by LIGO in detecting gravitational waves?\n",
      "The Answer:  LIGO faces challenges such as noise sources (e.g., thermal fluctuations), external disturbances (e.g., traffic), and potential interferences (e.g., rifle fire) in detecting gravitational waves.\n",
      "==============================\n",
      "==============================\n",
      "Random Question 5 : \n",
      " What is the main advantage of using a QR decomposition in the Longstaff-Schwartz algorithm?\n",
      "The Answer:  Using a QR decomposition reduces the computational complexity of the SVD for long-and-thin matrices, resulting in improved performance in option pricing.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "data = pd.read_csv(\"NvidiaDocumentationQandApairs.csv\")\n",
    "print(data.shape)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"=\"*30)\n",
    "    random_question = random.choice(list(data[\"question\"]))\n",
    "    print(\"Random Question\", str(i+1), \": \\n\", random_question)\n",
    "    print(\"The Answer: \", list(data[\"answer\"][data[\"question\"] == random_question])[0])\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40185d5",
   "metadata": {},
   "source": [
    "# Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11488064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "model_dir = \"./peft-Nvidia-chatbot-checkpoint-local\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \n",
    "                                       model_dir, \n",
    "                                       torch_dtype=torch.float32,\n",
    "                                       is_trainable=False)\n",
    "\n",
    "\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", \n",
    "                                                       torch_dtype=torch.float32,)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "976895cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: what is cuda ?\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: a genus of flowering plants\n",
      "\n",
      "T5 Fine-Tuned:  cuda is a library of deep learning algorithms that uses a set of specialized algorithms to perform arithmetic operations on a computer.\n",
      "==============================\n",
      "You: why Nvidia Cuda is so useful ?\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: Nvidia Cuda is a proprietary software that allows developers to create their own games.\n",
      "\n",
      "T5 Fine-Tuned:  Nvidia Cuda is so useful because it allows you to use Cuda for a wide range of tasks, including storing data on the GPU, storing data on the CPU, and storing data on the GPU.\n",
      "==============================\n",
      "You: How parallelization is useful for training ?\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: Parallelization is useful for training in a variety of ways.\n",
      "\n",
      "T5 Fine-Tuned:  Parallelization is useful for training because it allows for more efficient training and improved efficiency.\n",
      "==============================\n",
      "You: Can Parallelization speed up training ?\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: yes\n",
      "\n",
      "T5 Fine-Tuned:  Parallelization can speed up training by allowing multiple applications to be run concurrently on the same device. This can be done using a single CPU or GPU.\n",
      "==============================\n",
      "You: Thats nice\n",
      "\n",
      "Answers: \n",
      "\n",
      "Original Model: I'm a fan of the samurai\n",
      "\n",
      "T5 Fine-Tuned:  Thats nice.\n",
      "==============================\n",
      "You: exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text = input(\"You: \")\n",
    "    if text == \"exit\":\n",
    "        break\n",
    "    start_prompt = '\\n\\n'\n",
    "    end_prompt = '\\n\\nAnswer: '\n",
    "    prompt = start_prompt + text + end_prompt\n",
    "    tokenized_statement = tokenizer([prompt], return_tensors= 'pt')\n",
    "    \n",
    "    \n",
    "\n",
    "    ## Fine tuned model\n",
    "    outputs_ft = peft_model.generate(input_ids=tokenized_statement[\"input_ids\"], \n",
    "                                  generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),\n",
    "                                    )\n",
    "    \n",
    "   \n",
    "    final_output_ft = tokenizer.decode(outputs_ft[0], skip_special_tokens=True)\n",
    "    ###############\n",
    "    \n",
    "    ## Original model\n",
    "    op = original_model.generate(input_ids=tokenized_statement[\"input_ids\"], \n",
    "                                      generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),\n",
    "                                        )\n",
    "    \n",
    "    \n",
    "    print(\"\\nAnswers: \")\n",
    "    print(\"\\nOriginal Model:\", tokenizer.decode(op[0], skip_special_tokens=True))\n",
    "\n",
    "    print(\"\\nT5 Fine-Tuned: \", final_output_ft)\n",
    "    print(\"=\"*30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b04679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
